<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
    <title>DeepStyleCam Project Page</title>
    <meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

    <script src="lib.js" type="text/javascript"></script>
    <script src="popup.js" type="text/javascript"></script>
    <link rel="stylesheet" type="text/css" href="css/style.css" />
    <style type="text/css" media="all">
        IMG {
            PADDING-RIGHT: 0px;
            PADDING-LEFT: 0px;
            FLOAT: right;
            PADDING-BOTTOM: 0px;
            PADDING-TOP: 0px
        }

        #primarycontent {
            MARGIN-LEFT: auto;
            ;
            WIDTH: expression(document.body.clientWidth > 1000? "1000px": "auto");
            MARGIN-RIGHT: auto;
            TEXT-ALIGN: left;
            max-width:
                1000px
        }

        BODY {
            TEXT-ALIGN: center
        }
    </style>

    <meta content="MSHTML 6.00.2800.1400" name="GENERATOR">
    <script src="b5m.js" id="b5mmain" type="text/javascript"></script>
</head>

<body>

    <div id="primarycontent">
        <center>
            <h1>DeepStyleCam: A Real-Time Style Transfer App on iOS</h1>
        </center>
        <center>
  <a href="https://negi111111.github.io/">Ryosuke Tanno</a>&nbsp;&nbsp;&nbsp;
  <a href="">Shin Matsuo</a>&nbsp;&nbsp;&nbsp;
<a href="">Wataru Shimoda</a>&nbsp;&nbsp;&nbsp;
  <a href="http://acc.cs.uec.ac.jp/yanai/index.html">Keiji Yanai</a></h2>
        </center>
        <center>
            <h2><a href="http://mm.cs.uec.ac.jp/e/">Department of Informatics, The University of Electro-Communication</a></h2>
        </center>
        <center>
            <h2>In Proc. of International MultiMedia Modeling Conference (MMM 2017)</h2>
        </center>
        <p></p>

        <h2 align='center'></h2>
        <table border="0" align="center" cellspacing="0" cellpadding="20">
            <td align="center" valign="middle">
                <iframe width="480" height="270" src="https://www.youtube.com/embed/Ut5WYGi5yRU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                <div style="width:480px; text-align:left; font-size:14px">Ryosuke Tanno made the above..</div>
            </td>
        </table>

        <p>
<h2>Abstract</h2>

        <div style="font-size:14px">
            <p>In this demo, we present a very fast CNN-based style transfer
            system running on normal iPhones. The proposed app can transfer
            multiple pre-trained styles to the video stream captured from the builtin
            camera of an iPhone around 140ms (7fps). We extended the network
            proposed as a real-time neural style transfer network by Johnson et al. [1]
            so that the network can learn multiple styles at the same time. In addition,
            we modified the CNN network so that the amount of computation is
            reduced one tenth compared to the original network. The very fast mobile
            implementation of the app are based on our paper [2] which describes several
            new ideas to implement CNN on mobile devices efficiently. Figure 1
            shows an example usage of DeepStyleCam which is running on an
            iPhone SE.</p>
    </div>

    <a href=""><img style="float: left; padding: 10px; PADDING-RIGHT: 30px;" alt="paper thumbnail" src="images/paper_thumbnail.jpg" width=170></a>
    <br>



    <h2>Paper</h2>
    <p><a href="***">Coming soon!!</a>,  2018. </p>

    <h2>Citation</h2>
    <p>Ryosuke Tanno, Shin Matsuo, Wataru Shimoda and Keiji Yanai. "DeepStyleCam: A Real-Time Style Transfer App on iOS", In Proc. of International MultiMedia Modeling Conference (MMM), 2017.
<a href="DeepStyleCam.txt">Bibtex</a>

</p>
    <br>
    <br>
    <br>
    <br>

    <h2 align='center'> Another Application</h2>
    <table border="0" cellspacing="0" cellpadding="20">
        <tr>
            <td align="center" valign="middle">
                <h2>Food Transfer Image Museum on HoloLens</h2>
                <p><iframe width="480" height="270" src="https://www.youtube.com/embed/IEyKQ1i-fd4" frameborder="0" allowfullscreen></iframe></p>
                <div style="width:480px; text-align:left; font-size:14px">Shu Naritomi made the above.</div>
            </td>

            <td align="center" valign="middle">
                <h2>Food Image-to-Image Translation using StarGAN</h2>
                <p><iframe width="480" height="270" src="https://www.youtube.com/embed/nUboBFyy4Cc" frameborder="0" allowfullscreen></iframe></p>
                <div style="width:480px; text-align:left; font-size:14px">Ryosuke Tanno made the above.</div>
            </td>

        </tr>
    </table>


    <br><br><br>
    <h2>Related Work</h2>

    <ul id='relatedwork'>
        <li>
            Y. Matsuda, H. Hoashi, and K. Yanai <a href=""><strong>"Recognition of multiple-food
                    images by detecting candidate regions"</strong></a>, in ICME 2012.
        </li>
        <li>
            O. Ronneberger, P. Fischer, and T. Brox <a href=""><strong>"U-Net: Convolutional Networks
                    for Biomedical Image Segmentation"</strong></a>, in Medical Image Computing and Computer-Assisted Intervention (MICCAI) 2015.
        </li>
        <li>
            J. Y. Zhu, T. Park, P. Isola, and A. A. Efros <a href=""><strong>"Unpaired Image-to-Image
                    Translation using Cycle-Consistent Adversarial Networks"</strong></a>, in ICCV 2017.
        </li>
    </ul>


    <br>
    <h2>Acknowledgement</h2>
    <p>This work was supported by JSPS KAKENHI Grant Number 15H05915, 17H01745, 17H05972, 17H06026 and 17H06100.</p>

</body>
</html>
