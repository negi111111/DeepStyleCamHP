<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
    <title>DeepStyleCam Project Page</title>
    <meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

    <script src="lib.js" type="text/javascript"></script>
    <script src="popup.js" type="text/javascript"></script>
    <link rel="stylesheet" type="text/css" href="css/style.css" />
    <style type="text/css" media="all">
        IMG {
            PADDING-RIGHT: 0px;
            PADDING-LEFT: 0px;
            FLOAT: right;
            PADDING-BOTTOM: 0px;
            PADDING-TOP: 0px
        }

        #primarycontent {
            MARGIN-LEFT: auto;
            ;
            WIDTH: expression(document.body.clientWidth > 1000? "1000px": "auto");
            MARGIN-RIGHT: auto;
            TEXT-ALIGN: left;
            max-width:
                1000px
        }

        BODY {
            TEXT-ALIGN: center
        }
    </style>

    <meta content="MSHTML 6.00.2800.1400" name="GENERATOR">
    <script src="b5m.js" id="b5mmain" type="text/javascript"></script>
</head>

<body>

    <div id="primarycontent">
        <center>
            <h1>DeepStyleCam: A Real-Time Style Transfer App on iOS</h1>
        </center>
        <center>
            <h2>
  <a href="https://negi111111.github.io/">Ryosuke Tanno</a>&nbsp;&nbsp;&nbsp;
  <a href="">Shin Matsuo</a>&nbsp;&nbsp;&nbsp;
  <a href="">Wataru Shimoda</a>&nbsp;&nbsp;&nbsp;
  <a href="http://acc.cs.uec.ac.jp/yanai/index.html">Keiji Yanai</a></h2>
        </center>
        <center>
            <h2><a href="http://mm.cs.uec.ac.jp/e/">Department of Informatics, The University of Electro-Communication</a></h2>
        </center>
        <center>
            <h2>In Proc. of International MultiMedia Modeling Conference (MMM 2017)</h2>
        </center>
        <p></p>

        <h2 align='center'></h2>
        <table border="0" align="center" cellspacing="0" cellpadding="20">
            <td align="center" valign="middle">
                <iframe width="480" height="270" src="https://www.youtube.com/embed/Ut5WYGi5yRU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                <div style="width:480px; text-align:left; font-size:14px">Ryosuke Tanno made the above..</div>
            </td>
            <td align="center" valign="middle">
                <iframe width="480" height="270" src="https://www.youtube.com/embed/ZwfBBYy5I10" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                <div style="width:480px; text-align:left; font-size:14px">Ryosuke Tanno made the above..</div>
            </td>
        </table>

        <p>
<h2>Abstract</h2>

        <div style="font-size:14px">
            <p>In this demo, we present a very fast CNN-based style transfer
            system running on normal iPhones. The proposed app can transfer
            multiple pre-trained styles to the video stream captured from the builtin
            camera of an iPhone around 140ms (7fps). We extended the network
            proposed as a real-time neural style transfer network by Johnson et al. [1]
            so that the network can learn multiple styles at the same time. In addition,
            we modified the CNN network so that the amount of computation is
            reduced one tenth compared to the original network. The very fast mobile
            implementation of the app are based on our paper [2] which describes several
            new ideas to implement CNN on mobile devices efficiently. Figure 1
            shows an example usage of DeepStyleCam which is running on an
            iPhone SE.</p>
    </div>

    <a href=""><img style="float: left; padding: 10px; PADDING-RIGHT: 30px;" alt="paper thumbnail" src="images/paper.png" width=170></a>
    <br>



    <h2>Paper</h2>
    <p><a href="170103tanno_0.pdf"></a>PDF, 2017. </p>

    <h2>Citation</h2>
    <p>Ryosuke Tanno, Shin Matsuo, Wataru Shimoda and Keiji Yanai. "DeepStyleCam: A Real-Time Style Transfer App on iOS", In Proc. of International MultiMedia Modeling Conference (MMM), 2017.
<a href="DeepStyleCam.txt">Bibtex</a>

</p>
    <br>
    <br>

    <br><br><br><br>



    <h2>Related Work</h2>

    <ul id='relatedwork'>
        <li>
            Johnson, J., Alahi, A., Fei, L.F <a href=""><strong>"Perceptual losses for real-time style transfer
            and super-resolution"</strong></a>, in ECCV 2016.
        </li>
        <li>
            Yanai, K., Tanno, R., Okamoto, K <a href=""><strong>"Efficient mobile implementation of a CNN-based
            object recognition system"</strong></a>, in ACM Multimedia 2016.
        </li>
        <li>
            Gatys, L.A., Ecker, A.S., Bethge, M <a href=""><strong>"Image style transfer using convolutional neural networks"</strong></a>, in CVPR 2016.
        </li>
                <li>
                    Dong, C., Loy, C.C., He, K., Tang, X <a href=""><strong>"Learning a deep convolutional network for
                    image super-resolution"</strong></a>, in ECCV 2014.
                </li>

                                <li>
                                    Iizuka, S., Simo-Serra, E., Ishikawa, H <a href=""><strong>"Let there be color!: joint end-to-end learning
                                    of global and local image priors for auto"</strong></a>, in SIGGRAPH 2016.
                                </li>
                                                <li>
                                                    Gatys, L.A., Bethge, M., Hertzmann, A., Shechtman, E <a href=""><strong>"Preserving color in neural
                                                    artistic style transfer"</strong></a>, in arXiv:1606.05897 2016.
                                                </li>
    </ul>


    <br>

</body>
</html>
